
RCA TOOL ‚Äî CORRECTIVE INSTRUCTION FOR INITIAL QUESTIONNAIRE DESIGN
==================================================================


CONTEXT:
--------
This instruction addresses the flaws observed during testing of the RCA tool‚Äôs initial questionnaire and timeline section, specifically in a case where:

Incident Description:
"motor failed all of sudden ... noise for a short period ... load reduced ... rotor had burnt mark"

PROBLEM:
--------
The system presented only two failure modes ‚Äî "Rotor Bar Crack" and "Winding Insulation Fail" ‚Äî both from the Evidence Library for Induction Motors.

However:
- The failure description clearly suggests thermal/electrical overload or winding damage.
- No other relevant failure modes (e.g., Bearing Seizure, Phase Loss, Rotor Grounding Fault) were included.
- The evidence prompting and AI clarification logic was missing.

As a result, the system behaved too narrowly ‚Äî or worse, statically ‚Äî even though the logic must be fully universal.

MANDATORY UNIVERSAL DESIGN RULE:
---------------------------------
‚ö†Ô∏è UNDER NO CIRCUMSTANCE SHOULD ANY FAILURE MODE OR QUESTION BE HARDCODED TO A PARTICULAR EQUIPMENT TYPE.

ALL logic must be dynamically driven by the Evidence Library and AI interpretation of user-submitted incident information.

CORRECT UNIVERSAL DESIGN LOGIC:
-------------------------------

Step 1: NLP-Based Keyword Extraction
------------------------------------
- Parse the user‚Äôs incident title and description
- Extract key terms: e.g., "burnt", "rotor", "sudden", "load drop"
- Use synonyms: burnt = thermal, failure = fault, noise = vibration/humming

Step 2: Dynamic Failure Mode Filtering
--------------------------------------
- Query Evidence Library for matching subtype
- THEN FILTER failure modes where:
  a) failure_mode_description OR
  b) failure_mode_tags
  MATCH the extracted incident keywords

DO NOT pre-load all failure modes by subtype. Only dynamically match based on description relevance.

Step 3: AI Clarification Layer
------------------------------
- If incident description is vague (e.g., ‚Äúsome noise‚Äù), insert clarifying questions:
  - ‚ÄúWas the noise mechanical or electrical?‚Äù
  - ‚ÄúAny alarms triggered?‚Äù
  - ‚ÄúWas the motor cooling functional?‚Äù
- This helps narrow the AI reasoning track before presenting evidence questions.

Step 4: Evidence Prompting Logic
--------------------------------
- For each matched failure mode, load associated required_evidence
- Prompt user to upload relevant files (PDF, image, Excel, CSV)
- Do this in the timeline/question stage (not only in a later upload step)

Example:
Failure Mode = Winding Insulation Failure ‚Üí Ask for:
- IR report
- Temperature trend
- Alarm log

Step 5: Confidence and Gap Handling
-----------------------------------
- If key evidence not uploaded or marked ‚ÄúNot Known‚Äù ‚Üí confidence score degrades
- AI can trigger a fallback question:
  ‚ÄúThe evidence provided is not sufficient to confirm winding insulation failure. Please upload IR report or winding temp trend if available.‚Äù

Step 6: User Control
--------------------
- Allow user to add more failure modes manually (e.g., ‚ÄúI also suspect rotor imbalance‚Äù)
- But default list must be filtered by the AI + library logic

FINAL NOTES FOR DEVELOPMENT TEAM:
---------------------------------
üö´ DO NOT HARDCODE any failure mode into the frontend or backend
üö´ DO NOT tie failure mode logic to fixed equipment names
‚úÖ All prompts, evidence needs, and scoring must be driven dynamically using:
  - User-entered incident text (parsed)
  - Matching logic with Evidence Library entries
  - Subtype-level equipment filtering

‚úÖ The above logic must apply universally to any Equipment Group ‚Üí Type ‚Üí Subtype
‚úÖ System must be scalable for all industrial assets, not just motors or pumps

END OF INSTRUCTION
